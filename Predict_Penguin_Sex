import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix, classification_report

# Load the data
data = pd.read_csv('penguins_size.csv')
print(data.head())
# Handling missing values
imputer = SimpleImputer(strategy='median')
data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']] = imputer.fit_transform(data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']])
data['sex'].fillna(data['sex'].mode()[0], inplace=True)

# Encoding categorical variables
le = LabelEncoder()
data['species'] = le.fit_transform(data['species'])
data['island'] = le.fit_transform(data['island'])
data['sex'] = le.fit_transform(data['sex'])
# Data visualization
sns.pairplot(data, hue='species')
plt.show()
# Splitting the data into train and test sets
X = data.drop('species', axis=1)
y = data['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scaling features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier

# Setup the models with GridSearchCV
model_params = {
    'DecisionTree': {
        'model': DecisionTreeClassifier(),
        'params': {'max_depth': [None, 10, 20, 30], 'criterion': ['gini', 'entropy']}
    },
    'RandomForest': {
        'model': RandomForestClassifier(),
        'params': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20], 'criterion': ['gini', 'entropy']}
    },
    'AdaBoost': {
        'model': AdaBoostClassifier(),
        'params': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}
    },
    'GradientBoosting': {
        'model': GradientBoostingClassifier(),
        'params': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 5, 10]}
    }
}

# Training and tuning models
for model_name, mp in model_params.items():
    grid = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
    grid.fit(X_train_scaled, y_train)
    print(f"{model_name} best parameters: {grid.best_params_}")
    mp['best_score'] = grid.best_score_
    mp['best_model'] = grid.best_estimator_
# Evaluate each model
for model_name, mp in model_params.items():
    y_pred = mp['best_model'].predict(X_test_scaled)
    print(f"Model: {model_name}")
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.show()# Assuming you have a RandomForest model as one of your models
rf_model = model_params['RandomForest']['best_model']
features = X.columns
importances = rf_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(10, 8))
plt.title('Feature Importances in RandomForest Model')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
import joblib

# Saving the RandomForest model as an example
joblib.dump(rf_model, 'RandomForest_Penguin_Model.pkl')

# To load your model later
loaded_rf_model = joblib.load('RandomForest_Penguin_Model.pkl')
